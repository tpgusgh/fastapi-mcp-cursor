import os
from fastmcp import FastMCP
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import (
    ChatGoogleGenerativeAI,
    GoogleGenerativeAIEmbeddings,
)
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

mcp = FastMCP("pdf-rag")

vectorstore = None
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

PROMPT = ChatPromptTemplate.from_template("""
PDF ê¸°ë°˜ ì‘ë‹µ

ì§ˆë¬¸: {question}

ë¬¸ë§¥:
{context}
""")


async def rag(question: str):
    retriever = vectorstore.as_retriever()
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    chain = (
        {"context": retriever, "question": lambda x: x["question"]}
        | PROMPT
        | llm
        | StrOutputParser()
    )
    return chain.invoke({"question": question})


@mcp.tool
def upload_pdf(file_path: str) -> str:
    """PDFë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    global vectorstore
    if not os.path.exists(file_path):
        return "âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    docs = PyPDFLoader(file_path).load()
    vectorstore = FAISS.from_documents(docs, embeddings)
    return f"ğŸ“š PDF ìƒ‰ì¸ ì™„ë£Œ ({len(docs)} í˜ì´ì§€)"


@mcp.tool
async def summarize() -> str:
    """ë¬¸ì„œ ì „ì²´ ìš”ì•½"""
    if vectorstore is None:
        return "ğŸ“‚ ë¨¼ì € upload_pdf í˜¸ì¶œí•´ì£¼ì„¸ìš”!"

    summary = await rag("ë¬¸ì„œë¥¼ í•µì‹¬ë§Œ ìš”ì•½")
    return "ğŸ“Œ Summary:\n" + summary


@mcp.tool
async def ask(question: str) -> str:
    """ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ"""
    if vectorstore is None:
        return "ğŸ“‚ ë¨¼ì € upload_pdf í˜¸ì¶œí•´ì£¼ì„¸ìš”!"

    answer = await rag(question)
    return "ğŸ’¬ Answer:\n" + answer


if __name__ == "__main__":
    mcp.run()
